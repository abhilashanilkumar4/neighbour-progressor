import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

data = pd.read_csv(" https://github.com/edyoda/data-science-complete-tutorial/blob/master/Data/house_rental_data.csv.txt")

print(data.head())
print(data.isnull().sum())
print(data.describe())

# Data Preparation
X = data.drop("Price", axis=1)
y = data["Price"]

# Data Visualization
plt.figure(figsize=(8, 6))
plt.hist(y, bins=30, color='skyblue', edgecolor='black')
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.title("Distribution of House Prices")
plt.show()

# Correlation heatmap to see relationships between features and target variable
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')
plt.colorbar()
plt.xticks(range(len(correlation_matrix)), correlation_matrix.columns, rotation=45)
plt.yticks(range(len(correlation_matrix)), correlation_matrix.columns)
plt.title("Correlation Heatmap")
plt.show()

# Data Splitting for Training and Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Finding a better value of K using Cross-Validation
# different values of K and choose the one that gives the lowest mean squared error
k_values = list(range(1, 21))
mse_scores = []
for k in k_values:
    knn = KNeighborsRegressor(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    mse_scores.append(-np.mean(scores))

# Plotting Mean Squared Error vs. K
plt.figure(figsize=(8, 6))
plt.plot(k_values, mse_scores, marker='o', linestyle='-', color='b')
plt.xlabel("K (Number of Neighbors)")
plt.ylabel("Mean Squared Error")
plt.title("Mean Squared Error vs. K")
plt.xticks(k_values)
plt.grid()
plt.show()

# the best value of K
best_k = k_values[np.argmin(mse_scores)]
print("Best value of K:", best_k)

# the KNN Regressor with the best K value
knn = KNeighborsRegressor(n_neighbors=best_k)
knn.fit(X_train, y_train)

# Make predictions on the test set and evaluate the model
y_pred = knn.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error on test set:", mse)
